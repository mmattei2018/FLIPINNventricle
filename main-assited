import os
import tensorflow as tf
import numpy as np
import glob


class NonZeroMeanSquaredError(tf.keras.losses.Loss):
 
  def call(self, y_true, y_pred):
   
    mask = tf.math.logical_not(tf.math.equal(y_true, tf.constant([0], dtype = tf.float32)))

    #apply mask to ignore effect of values outside geometry
    mean = tf.reduce_mean(tf.math.abs(tf.boolean_mask(y_pred, mask) - tf.boolean_mask(y_true, mask)))
    #find how many values are not zero
    n = tf.math.count_nonzero(y_true, dtype=tf.float32)

 
    return tf.multiply(n, mean)

    #return tf.math.abs(tf.divide(tf.reduce_sum(tf.boolean_mask(y_pred, mask) - tf.boolean_mask(y_true, mask)),non_z))
    #divide error so its inversely proportional to number of data that is non zero, so we balance zero-dominated data to the rest
    #return 1000* tf.multiply(mean, tf.divide(1, n))
   
def conv_block(inputs, filters, kernel_size, act1 = 'PReLU', act2 = 'PReLU'):
    initializer = tf.keras.initializers.HeNormal()
    x = tf.keras.layers.Conv3D(filters, kernel_size, activation=act1,kernel_initializer = initializer , padding='same')(inputs)
    x = tf.keras.layers.Conv3D(filters, kernel_size, activation=act2, kernel_initializer = initializer, padding='same')(x)
    return x

def upsample_block(inputs, skip_connections, filters, kernel_size, act1 = 'PReLU', act2 = 'PReLU'):
    initializer = tf.keras.initializers.HeNormal()
    x = tf.keras.layers.Conv3DTranspose(filters, kernel_size, kernel_initializer = initializer, strides=(2, 2, 2), activation='PReLU', padding='same')(inputs)
    x = tf.keras.layers.concatenate([x, skip_connections], axis=-1)
    x = conv_block(x, filters, kernel_size, act1, act2)
    return x

def build_3d_unet(input_shape , num_classes):


   
    k1=(3,3,3)
    k2=(2,2,2)
    inp = tf.keras.Input(input_shape)
    #inp2 = tf.keras.Input(input2_shape)
    #vel = tf.keras.Input((16, 16, 16, 128))

    # Downsample path
    #inp = tf.keras.layers.concatenate([inp1, inp2], axis=-1)
    conv1 = conv_block(inp,  16, k1)
    conv1= tf.keras.layers.Dropout(0.1)(conv1)
    pool1 = tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2))(conv1)
    pool1 = tf.keras.layers.BatchNormalization()(pool1)


    conv2 = conv_block(pool1, 32,k1)
    conv2 = tf.keras.layers.Dropout(0.1)(conv2)
    pool2 = tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2))(conv2)
    pool2 = tf.keras.layers.BatchNormalization()(pool2)

    conv3 = conv_block(pool2, 64, k2)
    conv3= tf.keras.layers.Dropout(0.1)(conv3)
    pool3 = tf.keras.layers.MaxPooling3D(pool_size=(2,2,2))(conv3)
    pool3 = tf.keras.layers.BatchNormalization()(pool3)

    # Bottom of U-Net
    conv4 = conv_block(pool3,64, k1)
    conv4= tf.keras.layers.Dropout(0.1)(conv4)

    #multiply by velocity
    #conv4 = tf.keras.layers.concatenate([conv4, vel], -1)

    # Upsample path
    up5 = upsample_block(conv4, conv3, 64, k2)
    up5 = tf.keras.layers.BatchNormalization()(up5)
    up5= tf.keras.layers.Dropout(0.1)(up5)
    up6 = upsample_block(up5, conv2, 32,k2)
    up6= tf.keras.layers.Dropout(0.1)(up6)
    up6 = tf.keras.layers.BatchNormalization()(up6)
    up7 = upsample_block(up6, conv1, filters = 3, kernel_size= k2)
    #up7 = tf.keras.layers.BatchNormalization()(up7)
    up7= tf.keras.layers.Dropout(0.1)(up7)
    
    #concatenate to sample flow profile for reassesment at end
    #up7 = tf.keras.layers.concatenate([up7, inp2], axis=-1)

    # Output layer
    outputs = tf.keras.layers.Conv3D(num_classes, (1, 1, 1), activation='linear')(up7)

    #outputs = normalizer(outputs)


    model = tf.keras.Model(inputs=inp, outputs=outputs)
    return model


# Example usage
input_shape = (128, 128, 128, 2) 
input2_shape = (128, 128, 128, 1) # Example input shape (64x64x64) with 1 channel
num_classes = 3  # Example number of output classes
all_paths = np.sort(glob.glob('../FLIPINN_data/*/grid.npy'))

#preparing data

y_data = np.array([np.load(file)[0] for file in all_paths])
#scale up
#y_data *=10


#building model
model = build_3d_unet(input_shape, num_classes)
model.summary()

lrate = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5,
                                                     patience = 20, min_lr = 0.0000001)
filepath = "../savedmodels/VentricleUnet/XYZ_Assisted_5Centerplanes_NonzMSE_HeInit_normin_ZCOMPONENT_NOSAMPLING.hdf5"
save = tf.keras.callbacks.ModelCheckpoint(
    filepath,
    monitor = 'val_loss')

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
              loss=NonZeroMeanSquaredError())



sampled_paths = np.sort(glob.glob('../FLIPINN_data/*/grid_sampled.npy'))
y_sampled = np.array([np.load(file) for file in sampled_paths])

component=2

#vels = np.array([np.ones([16, 16, 16, 128])*float(path.split('/')[2].split('_')[-1]) for path in all_paths])
inp = y_data[:,:, :,:, (component, -1)]
print(f'input shape: {inp.shape}')

#normalize dimensions
#inp = np.expand_dims(inp, axis =-1)



centerplane = input("Do you want to train the network exclusively with the centerplane? enter 'True' for yes, 'False' for no: ")
if centerplane:
    
    
    test_paths = np.sort(glob.glob('../FLIPINN_data/*_V8*/grid.npy'))
    test_idxs = np.where(np.isin(all_paths, test_paths))[0]
    #test_vels = vels[test_idxs]
    test_inp = inp[test_idxs]# *100
    #keep only centerplane of input points
    centerplane_mask = np.zeros_like(test_inp)
    centerplane_idx = int(centerplane_mask.shape[1]//2)
    plane_width = 2
    centerplane_mask[:, centerplane_idx - plane_width: centerplane_idx + plane_width, :,:,0] = 1
    centerplane_mask[:, :, :,:,1] = 1
    test_inp *= centerplane_mask
    test_out =y_data[test_idxs,:,:,:, :3]#[:,:,:,:, component]

   
    
    #test_out = np.expand_dims(test_out, axis =-1)
    #testgen = DataGenerator(test_inp, test_vels, test_out , batch_size = 20)

    val_paths = np.sort(glob.glob('../FLIPINN_data/*_V9*/grid.npy'))
    val_idxs = np.where(np.isin(all_paths, val_paths))[0]
    #val_vels = vels[val_idxs]
    val_inp = inp[val_idxs] #*100
    centerplane_mask = np.zeros_like(val_inp)
    centerplane_mask[:, centerplane_idx - plane_width: centerplane_idx + plane_width, :,:,0] = 1
    centerplane_mask[:, :, :,:,1] = 1
    val_inp *= centerplane_mask
    val_out =y_data[val_idxs,:,:,:, :3]#[:,:,:,:, component]
    # val_out = np.expand_dims(val_out, axis =-1)
    #valgen = DataGenerator(val_inp, val_vels, val_out , batch_size = 1)

    train_paths = np.sort(glob.glob('../FLIPINN_data/*/grid.npy'))
    train_paths = train_paths[~np.isin(train_paths, np.hstack((val_paths, test_paths)))]
    train_idxs = np.where(np.isin(all_paths, train_paths))[0]
    #train_vels = vels[train_idxs]
    train_inp = inp[train_idxs] #*100
    centerplane_mask = np.zeros_like(train_inp)
    centerplane_mask[:, centerplane_idx - plane_width: centerplane_idx + plane_width, :,:,0] = 1
    centerplane_mask[:, :, :,:,1] = 1
    train_inp *= centerplane_mask
    train_out =y_data[train_idxs,:,:,:, :3] #[:,:,:,:, component]
    # train_out = np.expand_dims(train_out, axis =-1)

else:
    test_paths = np.sort(glob.glob('../FLIPINN_data/*_V8*/grid.npy'))
    test_idxs = np.where(np.isin(all_paths, test_paths))[0]
    #test_vels = vels[test_idxs]
    test_inp = inp[test_idxs] #*100
    test_out =y_data[test_idxs,:,:,:, :-1][:,:,:,:, component]
    test_out = np.expand_dims(test_out, axis =-1)
    #testgen = DataGenerator(test_inp, test_vels, test_out , batch_size = 20)

    val_paths = np.sort(glob.glob('../FLIPINN_data/*_V9*/grid.npy'))
    val_idxs = np.where(np.isin(all_paths, val_paths))[0]
    #val_vels = vels[val_idxs]
    val_inp = inp[val_idxs] #*100
    val_out =y_data[val_idxs,:,:,:, :-1][:,:,:,:, component]
    val_out = np.expand_dims(val_out, axis =-1)
    #valgen = DataGenerator(val_inp, val_vels, val_out , batch_size = 1)

    train_paths = np.sort(np.array(glob.glob('../FLIPINN_data/*/grid.npy')))
    train_paths = train_paths[~np.isin(train_paths, np.hstack((val_paths, test_paths)))]
    train_idxs = np.where(np.isin(all_paths, train_paths))[0]
    #train_vels = vels[train_idxs]
    train_inp = inp[train_idxs]# *100
    train_out =y_data[train_idxs,:,:,:, :-1] [:,:,:,:, component]
    train_out = np.expand_dims(train_out, axis =-1)
#traingen = DataGenerator(train_inp, train_vels, train_out , batch_size = 1)
model.fit(x = train_inp, y = train_out, validation_data = (val_inp, val_out),batch_size = 3, epochs = 25000, callbacks = [save], workers=20, use_multiprocessing = True)
